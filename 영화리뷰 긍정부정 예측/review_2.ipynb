{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "243aa3d2-1239-4517-a8d9-ef161270979e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\omw.zip.\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\omw-1.4.zip.\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet2021.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet31.zip.\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a732db79-4a07-462d-807b-7428bfb5f1e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>영상이나 음악이 이쁘다 해도 미화시킨 불륜일뿐</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>히치콕이 이 영화를 봤다면 분명 박수를 쳤을듯...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>괜찮은 음악영화가 또 나왔군요!!! 따뜻한 겨울이 될 것 같아요~</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>아무래도 20년도지난작품이라 지금보기는너무유치하다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>지금까지의 영화들이 그랬듯. 이 영화역시 일본에 대한 미화는 여전하다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                 document  label\n",
       "0   1                영상이나 음악이 이쁘다 해도 미화시킨 불륜일뿐      0\n",
       "1   2             히치콕이 이 영화를 봤다면 분명 박수를 쳤을듯...      1\n",
       "2   3     괜찮은 음악영화가 또 나왔군요!!! 따뜻한 겨울이 될 것 같아요~      1\n",
       "3   4              아무래도 20년도지난작품이라 지금보기는너무유치하다      0\n",
       "4   5  지금까지의 영화들이 그랬듯. 이 영화역시 일본에 대한 미화는 여전하다.      0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"G:/DACON/영화리뷰/train.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e756795b-f1d2-4118-b34e-38515578b533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "document    0\n",
       "label       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1180bb7e-3123-4718-a3f3-c35245eee22b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2189</td>\n",
       "      <td>2190</td>\n",
       "      <td>뻔한 스토리와 결말.평 좋은게 이해안감.조조로보는게 돈아깝지 않을듯.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024</td>\n",
       "      <td>2025</td>\n",
       "      <td>전편에 비해 정말 재미없다 -_- 잔인함도약하고 포스도 떨어진다</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4766</td>\n",
       "      <td>4767</td>\n",
       "      <td>도입부만 좋고 그 이후엔 망작</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>893</td>\n",
       "      <td>894</td>\n",
       "      <td>평론가 네이놈들!! 평점이 어찌 그따위냐?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31</td>\n",
       "      <td>32</td>\n",
       "      <td>현실적이지도, 그렇다고 환상적이지도 않은 불행한 로맨틱 코미디.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3745</th>\n",
       "      <td>1811</td>\n",
       "      <td>1812</td>\n",
       "      <td>재미읍다..말장난만 겁나 하고..</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3746</th>\n",
       "      <td>4823</td>\n",
       "      <td>4824</td>\n",
       "      <td>슬픈여운이 너무 길어서 슬픈영화.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3747</th>\n",
       "      <td>2128</td>\n",
       "      <td>2129</td>\n",
       "      <td>그냥 드라마 수준의 가벼운 명랑소녀힙합물. 제시카 알바 밀어주기 영화.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3748</th>\n",
       "      <td>3893</td>\n",
       "      <td>3894</td>\n",
       "      <td>역시나 작품성을 기대하긴 어렵다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3749</th>\n",
       "      <td>3086</td>\n",
       "      <td>3087</td>\n",
       "      <td>이거 등급 매긴 인간이나 돈남아도는거 같은 제작자나 감독이나 참 ㅉㅉ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3750 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index    id                                 document  label\n",
       "0      2189  2190   뻔한 스토리와 결말.평 좋은게 이해안감.조조로보는게 돈아깝지 않을듯.      0\n",
       "1      2024  2025      전편에 비해 정말 재미없다 -_- 잔인함도약하고 포스도 떨어진다      0\n",
       "2      4766  4767                         도입부만 좋고 그 이후엔 망작      0\n",
       "3       893   894                  평론가 네이놈들!! 평점이 어찌 그따위냐?      1\n",
       "4        31    32      현실적이지도, 그렇다고 환상적이지도 않은 불행한 로맨틱 코미디.      0\n",
       "...     ...   ...                                      ...    ...\n",
       "3745   1811  1812                       재미읍다..말장난만 겁나 하고..      0\n",
       "3746   4823  4824                       슬픈여운이 너무 길어서 슬픈영화.      1\n",
       "3747   2128  2129  그냥 드라마 수준의 가벼운 명랑소녀힙합물. 제시카 알바 밀어주기 영화.      0\n",
       "3748   3893  3894                       역시나 작품성을 기대하긴 어렵다.      0\n",
       "3749   3086  3087   이거 등급 매긴 인간이나 돈남아도는거 같은 제작자나 감독이나 참 ㅉㅉ      0\n",
       "\n",
       "[3750 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, val = train_test_split(data)\n",
    "train.reset_index(inplace = True)\n",
    "val.reset_index(inplace = True)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b415378-3dd8-446e-a5be-2120e53b3112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2189</td>\n",
       "      <td>2190</td>\n",
       "      <td>뻔한 스토리와 결말.평 좋은게 이해안감.조조로보는게 돈아깝지 않을듯.</td>\n",
       "      <td>0</td>\n",
       "      <td>뻔한 스토리와 결말평 좋은게 이해안감조조로보는게 돈아깝지 않을듯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024</td>\n",
       "      <td>2025</td>\n",
       "      <td>전편에 비해 정말 재미없다 -_- 잔인함도약하고 포스도 떨어진다</td>\n",
       "      <td>0</td>\n",
       "      <td>전편에 비해 정말 재미없다  잔인함도약하고 포스도 떨어진다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4766</td>\n",
       "      <td>4767</td>\n",
       "      <td>도입부만 좋고 그 이후엔 망작</td>\n",
       "      <td>0</td>\n",
       "      <td>도입부만 좋고 그 이후엔 망작</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>893</td>\n",
       "      <td>894</td>\n",
       "      <td>평론가 네이놈들!! 평점이 어찌 그따위냐?</td>\n",
       "      <td>1</td>\n",
       "      <td>평론가 네이놈들 평점이 어찌 그따위냐</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31</td>\n",
       "      <td>32</td>\n",
       "      <td>현실적이지도, 그렇다고 환상적이지도 않은 불행한 로맨틱 코미디.</td>\n",
       "      <td>0</td>\n",
       "      <td>현실적이지도 그렇다고 환상적이지도 않은 불행한 로맨틱 코미디</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index    id                                document  label  \\\n",
       "0   2189  2190  뻔한 스토리와 결말.평 좋은게 이해안감.조조로보는게 돈아깝지 않을듯.      0   \n",
       "1   2024  2025     전편에 비해 정말 재미없다 -_- 잔인함도약하고 포스도 떨어진다      0   \n",
       "2   4766  4767                        도입부만 좋고 그 이후엔 망작      0   \n",
       "3    893   894                 평론가 네이놈들!! 평점이 어찌 그따위냐?      1   \n",
       "4     31    32     현실적이지도, 그렇다고 환상적이지도 않은 불행한 로맨틱 코미디.      0   \n",
       "\n",
       "                          preprocessed  \n",
       "0  뻔한 스토리와 결말평 좋은게 이해안감조조로보는게 돈아깝지 않을듯  \n",
       "1     전편에 비해 정말 재미없다  잔인함도약하고 포스도 떨어진다  \n",
       "2                     도입부만 좋고 그 이후엔 망작  \n",
       "3                 평론가 네이놈들 평점이 어찌 그따위냐  \n",
       "4    현실적이지도 그렇다고 환상적이지도 않은 불행한 로맨틱 코미디  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['preprocessed'] = train['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "227f79ac-773a-4db3-9b15-38e62ac2c55d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2189</td>\n",
       "      <td>2190</td>\n",
       "      <td>뻔한 스토리와 결말.평 좋은게 이해안감.조조로보는게 돈아깝지 않을듯.</td>\n",
       "      <td>0</td>\n",
       "      <td>뻔한 스토리와 결말평 좋은게 이해안감조조로보는게 돈아깝지 않을듯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024</td>\n",
       "      <td>2025</td>\n",
       "      <td>전편에 비해 정말 재미없다 -_- 잔인함도약하고 포스도 떨어진다</td>\n",
       "      <td>0</td>\n",
       "      <td>전편에 비해 정말 재미없다 잔인함도약하고 포스도 떨어진다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4766</td>\n",
       "      <td>4767</td>\n",
       "      <td>도입부만 좋고 그 이후엔 망작</td>\n",
       "      <td>0</td>\n",
       "      <td>도입부만 좋고 그 이후엔 망작</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>893</td>\n",
       "      <td>894</td>\n",
       "      <td>평론가 네이놈들!! 평점이 어찌 그따위냐?</td>\n",
       "      <td>1</td>\n",
       "      <td>평론가 네이놈들 평점이 어찌 그따위냐</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31</td>\n",
       "      <td>32</td>\n",
       "      <td>현실적이지도, 그렇다고 환상적이지도 않은 불행한 로맨틱 코미디.</td>\n",
       "      <td>0</td>\n",
       "      <td>현실적이지도 그렇다고 환상적이지도 않은 불행한 로맨틱 코미디</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index    id                                document  label  \\\n",
       "0   2189  2190  뻔한 스토리와 결말.평 좋은게 이해안감.조조로보는게 돈아깝지 않을듯.      0   \n",
       "1   2024  2025     전편에 비해 정말 재미없다 -_- 잔인함도약하고 포스도 떨어진다      0   \n",
       "2   4766  4767                        도입부만 좋고 그 이후엔 망작      0   \n",
       "3    893   894                 평론가 네이놈들!! 평점이 어찌 그따위냐?      1   \n",
       "4     31    32     현실적이지도, 그렇다고 환상적이지도 않은 불행한 로맨틱 코미디.      0   \n",
       "\n",
       "                          preprocessed  \n",
       "0  뻔한 스토리와 결말평 좋은게 이해안감조조로보는게 돈아깝지 않을듯  \n",
       "1      전편에 비해 정말 재미없다 잔인함도약하고 포스도 떨어진다  \n",
       "2                     도입부만 좋고 그 이후엔 망작  \n",
       "3                 평론가 네이놈들 평점이 어찌 그따위냐  \n",
       "4    현실적이지도 그렇다고 환상적이지도 않은 불행한 로맨틱 코미디  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['preprocessed'] = train['preprocessed'].str.replace(\" +\", \" \") \n",
    "train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bf4ef3b-91e3-49ba-a2a0-7a63197884e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed</th>\n",
       "      <th>tokenized_stem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2189</td>\n",
       "      <td>2190</td>\n",
       "      <td>뻔한 스토리와 결말.평 좋은게 이해안감.조조로보는게 돈아깝지 않을듯.</td>\n",
       "      <td>0</td>\n",
       "      <td>뻔한 스토리와 결말평 좋은게 이해안감조조로보는게 돈아깝지 않을듯</td>\n",
       "      <td>뻔하다 스토리 와 결말 평 좋다 은 게 이해 안감 조조 로 보다 돈 아깝다 않다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024</td>\n",
       "      <td>2025</td>\n",
       "      <td>전편에 비해 정말 재미없다 -_- 잔인함도약하고 포스도 떨어진다</td>\n",
       "      <td>0</td>\n",
       "      <td>전편에 비해 정말 재미없다 잔인함도약하고 포스도 떨어진다</td>\n",
       "      <td>전편 에 비다 정말 재미없다 잔인함 도약 하고 포스 도 떨어지다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4766</td>\n",
       "      <td>4767</td>\n",
       "      <td>도입부만 좋고 그 이후엔 망작</td>\n",
       "      <td>0</td>\n",
       "      <td>도입부만 좋고 그 이후엔 망작</td>\n",
       "      <td>도입 부 만 좋다 그 이후 엔 망작</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>893</td>\n",
       "      <td>894</td>\n",
       "      <td>평론가 네이놈들!! 평점이 어찌 그따위냐?</td>\n",
       "      <td>1</td>\n",
       "      <td>평론가 네이놈들 평점이 어찌 그따위냐</td>\n",
       "      <td>평론가 네 이 놈 들 평점 이 어찌 그따위 냐</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31</td>\n",
       "      <td>32</td>\n",
       "      <td>현실적이지도, 그렇다고 환상적이지도 않은 불행한 로맨틱 코미디.</td>\n",
       "      <td>0</td>\n",
       "      <td>현실적이지도 그렇다고 환상적이지도 않은 불행한 로맨틱 코미디</td>\n",
       "      <td>현실 적 이 지도 그렇다고 환상 적 이 지도 않다 불행하다 로맨틱 코미디</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index    id                                document  label  \\\n",
       "0   2189  2190  뻔한 스토리와 결말.평 좋은게 이해안감.조조로보는게 돈아깝지 않을듯.      0   \n",
       "1   2024  2025     전편에 비해 정말 재미없다 -_- 잔인함도약하고 포스도 떨어진다      0   \n",
       "2   4766  4767                        도입부만 좋고 그 이후엔 망작      0   \n",
       "3    893   894                 평론가 네이놈들!! 평점이 어찌 그따위냐?      1   \n",
       "4     31    32     현실적이지도, 그렇다고 환상적이지도 않은 불행한 로맨틱 코미디.      0   \n",
       "\n",
       "                          preprocessed  \\\n",
       "0  뻔한 스토리와 결말평 좋은게 이해안감조조로보는게 돈아깝지 않을듯   \n",
       "1      전편에 비해 정말 재미없다 잔인함도약하고 포스도 떨어진다   \n",
       "2                     도입부만 좋고 그 이후엔 망작   \n",
       "3                 평론가 네이놈들 평점이 어찌 그따위냐   \n",
       "4    현실적이지도 그렇다고 환상적이지도 않은 불행한 로맨틱 코미디   \n",
       "\n",
       "                                 tokenized_stem  \n",
       "0  뻔하다 스토리 와 결말 평 좋다 은 게 이해 안감 조조 로 보다 돈 아깝다 않다  \n",
       "1           전편 에 비다 정말 재미없다 잔인함 도약 하고 포스 도 떨어지다  \n",
       "2                           도입 부 만 좋다 그 이후 엔 망작  \n",
       "3                     평론가 네 이 놈 들 평점 이 어찌 그따위 냐  \n",
       "4      현실 적 이 지도 그렇다고 환상 적 이 지도 않다 불행하다 로맨틱 코미디  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "tokenized = []\n",
    "for sentence in train['preprocessed']:\n",
    "    tokens = okt.morphs(sentence, stem = True)\n",
    "    tokenize = ' '.join(tokens)\n",
    "    tokenized.append(tokenize)\n",
    "\n",
    "train['tokenized_stem'] = pd.DataFrame(tokenized)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fedc766-a2f1-4d28-8fe0-6eacbb571932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed</th>\n",
       "      <th>tokenized_stem</th>\n",
       "      <th>main_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2189</td>\n",
       "      <td>2190</td>\n",
       "      <td>뻔한 스토리와 결말.평 좋은게 이해안감.조조로보는게 돈아깝지 않을듯.</td>\n",
       "      <td>0</td>\n",
       "      <td>뻔한 스토리와 결말평 좋은게 이해안감조조로보는게 돈아깝지 않을듯</td>\n",
       "      <td>뻔하다 스토리 와 결말 평 좋다 은 게 이해 안감 조조 로 보다 돈 아깝다 않다</td>\n",
       "      <td>뻔한 스토리 결말 평 좋 은 이해 안감 조조 보는게 돈 아깝지 않을듯</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024</td>\n",
       "      <td>2025</td>\n",
       "      <td>전편에 비해 정말 재미없다 -_- 잔인함도약하고 포스도 떨어진다</td>\n",
       "      <td>0</td>\n",
       "      <td>전편에 비해 정말 재미없다 잔인함도약하고 포스도 떨어진다</td>\n",
       "      <td>전편 에 비다 정말 재미없다 잔인함 도약 하고 포스 도 떨어지다</td>\n",
       "      <td>전편 비해 정말 재미없다 잔인함 도약 포스 떨어진다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4766</td>\n",
       "      <td>4767</td>\n",
       "      <td>도입부만 좋고 그 이후엔 망작</td>\n",
       "      <td>0</td>\n",
       "      <td>도입부만 좋고 그 이후엔 망작</td>\n",
       "      <td>도입 부 만 좋다 그 이후 엔 망작</td>\n",
       "      <td>도입 부 좋고 그 이후 망작</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>893</td>\n",
       "      <td>894</td>\n",
       "      <td>평론가 네이놈들!! 평점이 어찌 그따위냐?</td>\n",
       "      <td>1</td>\n",
       "      <td>평론가 네이놈들 평점이 어찌 그따위냐</td>\n",
       "      <td>평론가 네 이 놈 들 평점 이 어찌 그따위 냐</td>\n",
       "      <td>평론가 놈 평점 어찌 그따위</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31</td>\n",
       "      <td>32</td>\n",
       "      <td>현실적이지도, 그렇다고 환상적이지도 않은 불행한 로맨틱 코미디.</td>\n",
       "      <td>0</td>\n",
       "      <td>현실적이지도 그렇다고 환상적이지도 않은 불행한 로맨틱 코미디</td>\n",
       "      <td>현실 적 이 지도 그렇다고 환상 적 이 지도 않다 불행하다 로맨틱 코미디</td>\n",
       "      <td>현실 지도 환상 지도 않은 불행한 로맨틱 코미디</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index    id                                document  label  \\\n",
       "0   2189  2190  뻔한 스토리와 결말.평 좋은게 이해안감.조조로보는게 돈아깝지 않을듯.      0   \n",
       "1   2024  2025     전편에 비해 정말 재미없다 -_- 잔인함도약하고 포스도 떨어진다      0   \n",
       "2   4766  4767                        도입부만 좋고 그 이후엔 망작      0   \n",
       "3    893   894                 평론가 네이놈들!! 평점이 어찌 그따위냐?      1   \n",
       "4     31    32     현실적이지도, 그렇다고 환상적이지도 않은 불행한 로맨틱 코미디.      0   \n",
       "\n",
       "                          preprocessed  \\\n",
       "0  뻔한 스토리와 결말평 좋은게 이해안감조조로보는게 돈아깝지 않을듯   \n",
       "1      전편에 비해 정말 재미없다 잔인함도약하고 포스도 떨어진다   \n",
       "2                     도입부만 좋고 그 이후엔 망작   \n",
       "3                 평론가 네이놈들 평점이 어찌 그따위냐   \n",
       "4    현실적이지도 그렇다고 환상적이지도 않은 불행한 로맨틱 코미디   \n",
       "\n",
       "                                 tokenized_stem  \\\n",
       "0  뻔하다 스토리 와 결말 평 좋다 은 게 이해 안감 조조 로 보다 돈 아깝다 않다   \n",
       "1           전편 에 비다 정말 재미없다 잔인함 도약 하고 포스 도 떨어지다   \n",
       "2                           도입 부 만 좋다 그 이후 엔 망작   \n",
       "3                     평론가 네 이 놈 들 평점 이 어찌 그따위 냐   \n",
       "4      현실 적 이 지도 그렇다고 환상 적 이 지도 않다 불행하다 로맨틱 코미디   \n",
       "\n",
       "                                 main_pos  \n",
       "0  뻔한 스토리 결말 평 좋 은 이해 안감 조조 보는게 돈 아깝지 않을듯  \n",
       "1            전편 비해 정말 재미없다 잔인함 도약 포스 떨어진다  \n",
       "2                         도입 부 좋고 그 이후 망작  \n",
       "3                         평론가 놈 평점 어찌 그따위  \n",
       "4              현실 지도 환상 지도 않은 불행한 로맨틱 코미디  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_pos = []\n",
    "for sentence in train['document']:\n",
    "    pos = okt.pos(sentence)\n",
    "    main_words = [word_pos[0] for word_pos in pos if word_pos[1] in (\"Noun\", \"Adverb\", \"Adjective\", \"Verb\")]\n",
    "    main_words_str = ' '.join(main_words)\n",
    "    main_pos.append(main_words_str)\n",
    "\n",
    "train[\"main_pos\"] = pd.DataFrame(main_pos)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3781ae2-b086-40cb-920e-09791163a280",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train.main_pos\n",
    "y_train = train.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a603549-9fdd-4cdb-b773-78d05855e431",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(x_train)\n",
    "x_train_vec = vectorizer.transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e9ee351-0eaf-43d9-9b6e-ce85e9bc3297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6fbf556d-bf8c-417e-9b48-4b2f934d379c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed</th>\n",
       "      <th>tokenized_stem</th>\n",
       "      <th>main_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3696</td>\n",
       "      <td>3697</td>\n",
       "      <td>최고의영화,말이필요없다..눈물,감동..재미..100%</td>\n",
       "      <td>1</td>\n",
       "      <td>최고의영화말이필요없다눈물감동재미</td>\n",
       "      <td>최고 의 영화 말 이 필요없다 눈물 감동 재미</td>\n",
       "      <td>최고 영화 말 필요없다 눈물 감동 재미</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>가슴 찡했던, 오래 기억에남을 영화!!</td>\n",
       "      <td>1</td>\n",
       "      <td>가슴 찡했던 오래 기억에남을 영화</td>\n",
       "      <td>가슴 찡하다 오래 기억 에 남 을 영화</td>\n",
       "      <td>가슴 찡했던 오래 기억 남 영화</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4593</td>\n",
       "      <td>4594</td>\n",
       "      <td>이딴 영화가 있을줄은 생각 못함</td>\n",
       "      <td>0</td>\n",
       "      <td>이딴 영화가 있을줄은 생각 못함</td>\n",
       "      <td>이딴 영화 가 있다 생각 못 하다</td>\n",
       "      <td>영화 있을줄은 생각 함</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1670</td>\n",
       "      <td>1671</td>\n",
       "      <td>이블데드와 함께 쌍벽을 이루는 공포의 고전^^ 개인적으로2편 좋아함~</td>\n",
       "      <td>1</td>\n",
       "      <td>이블데드와 함께 쌍벽을 이루는 공포의 고전 개인적으로편 좋아함</td>\n",
       "      <td>이블데드 와 함께 쌍벽 을 이루다 공포 의 고전 개인 적 으로 편 좋아하다</td>\n",
       "      <td>이블데드 함께 쌍벽 이루는 공포 고전 개인 편 좋아함</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1729</td>\n",
       "      <td>1730</td>\n",
       "      <td>난 잘 모르겠다. 기억나는 장면도 없고 그냥 지루.</td>\n",
       "      <td>0</td>\n",
       "      <td>난 잘 모르겠다 기억나는 장면도 없고 그냥 지루</td>\n",
       "      <td>난 자다 모르다 기억나다 장면 도 없다 그냥 지루</td>\n",
       "      <td>난 잘 모르겠다 기억나는 장면 없고 그냥 지루</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index    id                                document  label  \\\n",
       "0   3696  3697           최고의영화,말이필요없다..눈물,감동..재미..100%      1   \n",
       "1      8     9                   가슴 찡했던, 오래 기억에남을 영화!!      1   \n",
       "2   4593  4594                       이딴 영화가 있을줄은 생각 못함      0   \n",
       "3   1670  1671  이블데드와 함께 쌍벽을 이루는 공포의 고전^^ 개인적으로2편 좋아함~      1   \n",
       "4   1729  1730            난 잘 모르겠다. 기억나는 장면도 없고 그냥 지루.      0   \n",
       "\n",
       "                         preprocessed  \\\n",
       "0                   최고의영화말이필요없다눈물감동재미   \n",
       "1                  가슴 찡했던 오래 기억에남을 영화   \n",
       "2                   이딴 영화가 있을줄은 생각 못함   \n",
       "3  이블데드와 함께 쌍벽을 이루는 공포의 고전 개인적으로편 좋아함   \n",
       "4          난 잘 모르겠다 기억나는 장면도 없고 그냥 지루   \n",
       "\n",
       "                              tokenized_stem                       main_pos  \n",
       "0                  최고 의 영화 말 이 필요없다 눈물 감동 재미          최고 영화 말 필요없다 눈물 감동 재미  \n",
       "1                      가슴 찡하다 오래 기억 에 남 을 영화              가슴 찡했던 오래 기억 남 영화  \n",
       "2                         이딴 영화 가 있다 생각 못 하다                   영화 있을줄은 생각 함  \n",
       "3  이블데드 와 함께 쌍벽 을 이루다 공포 의 고전 개인 적 으로 편 좋아하다  이블데드 함께 쌍벽 이루는 공포 고전 개인 편 좋아함  \n",
       "4                난 자다 모르다 기억나다 장면 도 없다 그냥 지루      난 잘 모르겠다 기억나는 장면 없고 그냥 지루  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val['preprocessed'] = val['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") \n",
    "val['preprocessed'] = val['preprocessed'].str.replace(\" +\", \" \") \n",
    "\n",
    "tokenized = []\n",
    "for sentence in val['preprocessed']:\n",
    "    tokens = okt.morphs(sentence, stem = True) \n",
    "    tokenize = \" \".join(tokens)\n",
    "    tokenized.append(tokenize)\n",
    "val[\"tokenized_stem\"] = pd.DataFrame(tokenized)\n",
    "\n",
    "main_pos = []\n",
    "for sentence in val['document']:\n",
    "    pos = okt.pos(sentence)\n",
    "    main_words = [word_pos[0] for word_pos in pos if word_pos[1] in (\"Noun\", \"Adverb\", \"Adjective\", \"Verb\")]\n",
    "    main_words_str = \" \".join(main_words)\n",
    "    main_pos.append(main_words_str)\n",
    "val[\"main_pos\"] = pd.DataFrame(main_pos)\n",
    "\n",
    "val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e72f5a44-9fd9-4695-bf7f-1e48b68784d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = val.main_pos\n",
    "y_val = val.label\n",
    "\n",
    "x_val_vec = vectorizer.transform(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df6b22a3-9330-4db1-910b-2262fb1510b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_val_vec)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6648b19b-eaca-48c2-bcfc-39be4d0de19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8352\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.accuracy_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dbcc477b-b6aa-436a-b0a9-6ac2e06774b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>시간 때우기 좋은 영화 지루함</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>훈훈한 정이 느껴지는 영화! 가족끼리 드라마 보듯이 보면 딱~!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Childhood fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>멋있는 영화입니다. 잊을 수 없는!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>너무 감동적이네요 펑펑 울었습니다</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                             document\n",
       "0   1                     시간 때우기 좋은 영화 지루함\n",
       "1   2  훈훈한 정이 느껴지는 영화! 가족끼리 드라마 보듯이 보면 딱~!\n",
       "2   3                    Childhood fantasy\n",
       "3   4                  멋있는 영화입니다. 잊을 수 없는!\n",
       "4   5                   너무 감동적이네요 펑펑 울었습니다"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"G:/DACON/영화리뷰/test.csv\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "41f223a1-aea3-4b4c-8ffb-6827a94a921b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>preprocessed</th>\n",
       "      <th>tokenized_stem</th>\n",
       "      <th>main_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>시간 때우기 좋은 영화 지루함</td>\n",
       "      <td>시간 때우기 좋은 영화 지루함</td>\n",
       "      <td>시간 때우다 좋다 영화 지루함</td>\n",
       "      <td>시간 때우기 좋은 영화 지루함</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>훈훈한 정이 느껴지는 영화! 가족끼리 드라마 보듯이 보면 딱~!</td>\n",
       "      <td>훈훈한 정이 느껴지는 영화 가족끼리 드라마 보듯이 보면 딱</td>\n",
       "      <td>훈훈하다 정이 느껴지다 영화 가족 끼리 드라마 보다 보다 딱</td>\n",
       "      <td>훈훈한 정이 느껴지는 영화 가족 끼리 드라마 보듯이 보면 딱</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Childhood fantasy</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>멋있는 영화입니다. 잊을 수 없는!</td>\n",
       "      <td>멋있는 영화입니다 잊을 수 없는</td>\n",
       "      <td>멋있다 영화 이다 잊다 수 없다</td>\n",
       "      <td>멋있는 영화 입니다 잊을 수 없는</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>너무 감동적이네요 펑펑 울었습니다</td>\n",
       "      <td>너무 감동적이네요 펑펑 울었습니다</td>\n",
       "      <td>너무 감동 적다 펑펑 울다</td>\n",
       "      <td>너무 감동 적이네요 펑펑 울었습니다</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                             document                      preprocessed  \\\n",
       "0   1                     시간 때우기 좋은 영화 지루함                  시간 때우기 좋은 영화 지루함   \n",
       "1   2  훈훈한 정이 느껴지는 영화! 가족끼리 드라마 보듯이 보면 딱~!  훈훈한 정이 느껴지는 영화 가족끼리 드라마 보듯이 보면 딱   \n",
       "2   3                    Childhood fantasy                                     \n",
       "3   4                  멋있는 영화입니다. 잊을 수 없는!                 멋있는 영화입니다 잊을 수 없는   \n",
       "4   5                   너무 감동적이네요 펑펑 울었습니다                너무 감동적이네요 펑펑 울었습니다   \n",
       "\n",
       "                      tokenized_stem                           main_pos  \n",
       "0                   시간 때우다 좋다 영화 지루함                   시간 때우기 좋은 영화 지루함  \n",
       "1  훈훈하다 정이 느껴지다 영화 가족 끼리 드라마 보다 보다 딱  훈훈한 정이 느껴지는 영화 가족 끼리 드라마 보듯이 보면 딱  \n",
       "2                                                                        \n",
       "3                  멋있다 영화 이다 잊다 수 없다                 멋있는 영화 입니다 잊을 수 없는  \n",
       "4                     너무 감동 적다 펑펑 울다                너무 감동 적이네요 펑펑 울었습니다  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['preprocessed'] = test['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") \n",
    "test['preprocessed'] = test['preprocessed'].str.replace(\" +\", \" \")\n",
    "\n",
    "tokenized = []\n",
    "for sentence in test['preprocessed']:\n",
    "    tokens = okt.morphs(sentence, stem = True) \n",
    "    tokenize = \" \".join(tokens)\n",
    "    tokenized.append(tokenize)\n",
    "test[\"tokenized_stem\"] = pd.DataFrame(tokenized)\n",
    "\n",
    "main_pos = []\n",
    "for sentence in test['document']:\n",
    "    pos = okt.pos(sentence)\n",
    "    main_words = [word_pos[0] for word_pos in pos if word_pos[1] in (\"Noun\", \"Adverb\", \"Adjective\", \"Verb\")]\n",
    "    main_words_str = \" \".join(main_words)\n",
    "    main_pos.append(main_words_str)\n",
    "test[\"main_pos\"] = pd.DataFrame(main_pos)\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a8cfcd54-5dc0-4684-aa38-22228fee6b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "X_test = test.main_pos\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "pred_test = model.predict(X_test_vec)\n",
    "print(pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283c1c25-98c5-40ca-a68f-ea63de663980",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
